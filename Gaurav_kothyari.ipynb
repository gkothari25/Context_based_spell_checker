{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"'''\nAuthor - Gaurav Kothyari\nDate   - 29-09-2020\nVersion - Contexual_error_correction - version1\n'''\n\n!pip install wikipedia\n!pip install spacy\nimport spacy\nsp = spacy.load('en_core_web_sm')\n# import spacy\nimport wikipedia\n\n#text data for understanding the context of the data\nartificial_intelligence = wikipedia.page(\"Artificial Intelligence\").content\nmachine_learning = wikipedia.page(\"machine learning\").content\nDeep_learning = wikipedia.page(\"Deep learning\").content\nData_science = wikipedia.page(\"Data science\").content\ncomputer_Programming = wikipedia.page(\"computer Programming\").content\nMultiprocessing = wikipedia.page(\"Multiprocessing\").content\nsoftware_company = wikipedia.page(\"software company\").content\nsoftware_indistry = wikipedia.page(\"software indistry\").content\nrecruitment = wikipedia.page(\"recruitment\").content\nproblem_solving = wikipedia.page(\"problem solving\").content\nSoftware_deployment = wikipedia.page(\"Software deployment\").content\nSoftware_development = wikipedia.page(\"Software development\").content\nEmployee_relationship_management = wikipedia.page(\"Employee relationship management\").content\nBusiness_relations = wikipedia.page(\"Business relations\").content\nCustomer_service = wikipedia.page(\"Customer service\").content\nWorkplace_harassment = wikipedia.page(\"Workplace harassment\").content\nInformation_Technology_in_India = wikipedia.page(\"Information Technology in India\").content\n\nfrom nltk.tokenize import sent_tokenize\n\ndataset_gk = []\n\n\nartificial_intelligence =     [s.text for s in sp(artificial_intelligence).sents]\nmachine_learning =            [s.text for s in sp(machine_learning).sents]\nDeep_learning =               [s.text for s in sp(Deep_learning).sents]\nData_science =                [s.text for s in sp(Data_science).sents]\ncomputer_Programming =         [s.text for s in sp(computer_Programming).sents]\nMultiprocessing =          [s.text for s in sp(Multiprocessing).sents]\nsoftware_company =         [s.text for s in sp(software_company).sents]\nsoftware_indistry =       [s.text for s in sp(software_indistry).sents]\nrecruitment =           [s.text for s in sp(recruitment).sents]\nproblem_solving =        [s.text for s in sp(problem_solving).sents]\nSoftware_deployment =      [s.text for s in sp(Software_deployment).sents]\nSoftware_development =      [s.text for s in sp(Software_development).sents]\nEmployee_relationship_management =    [s.text for s in sp(Employee_relationship_management).sents]\nBusiness_relations =             [s.text for s in sp(Business_relations).sents]\nCustomer_service =                [s.text for s in sp(Customer_service).sents]\nWorkplace_harassment =          [s.text for s in sp(Workplace_harassment).sents]\nInformation_Technology_in_India =  [s.text for s in sp(Information_Technology_in_India).sents]\n\n\n\ndataset_gk.extend(artificial_intelligence)\ndataset_gk.extend(machine_learning)\ndataset_gk.extend(Deep_learning)\ndataset_gk.extend(Data_science)\ndataset_gk.extend(computer_Programming)\ndataset_gk.extend(Multiprocessing)\ndataset_gk.extend(software_company)\ndataset_gk.extend(software_indistry)\ndataset_gk.extend(recruitment)\ndataset_gk.extend(problem_solving)\ndataset_gk.extend(Software_deployment)\ndataset_gk.extend(Software_development)\ndataset_gk.extend(Employee_relationship_management)\ndataset_gk.extend(Business_relations)\ndataset_gk.extend(Customer_service)\ndataset_gk.extend(Workplace_harassment)\ndataset_gk.extend(Information_Technology_in_India)\n\n\n# print(dataset_gk)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# lets do the data preprocessing.\n# !pip install -U spacy\nimport spacy\nsp = spacy.load('en_core_web_sm')\n\n\nimport re\n\ndef preprocess1(str1):\n    \n    # apply some regular expression.\n    tr = str1\n    #remove non word character\n    pattern1 = r\"\\W\"\n    str1 = re.sub(pattern1,' ',str1)\n    \n    #remove isolated character\n    pattern2 =r'\\s+[a-zA-Z]\\s+'\n    str1 = re.sub(pattern2,' ',str1)\n    \n    #remove first single character\n    pattern3 =r\"^[a-zA-Z]\\s+\"\n    str1 = re.sub(pattern3,' ',str1)\n    \n    #remove multiple spaces with single space\n    pattern4 = r\"\\s+\"\n    str1 = re.sub(pattern4,' ',str1,flags=re.I)\n    \n    #lowercase string.lower\n    str1 = str1.lower()\n    \n    #split the sentence into tokens using space\n    #tokens= str1.split(' ')\n    tokens = [w for w in sp(str1)]\n    \n    all_stopwords = sp.Defaults.stop_words\n    s_words = [wd1.lemma_ for wd1 in tokens if wd1.text in all_stopwords]\n    \n    n_s_words = [wd for wd in tokens if wd.text not in all_stopwords]\n    \n    \n    #lemmatization\n    tokens = [word.lemma_ for word in n_s_words]\n    \n    #join the list\n    tokens_sent = ' '.join(tokens)\n    \n    token_word = tokens + s_words\n    \n    #print(token_word)\n    \n    return (token_word,tokens_sent)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the data\nmy_dictionary = []\ndata1 = []\nfor str1 in dataset_gk:\n    if str1!= '':\n        #print(str1)\n        temp_out = preprocess1(str1)\n        #print(temp_out)\n        data1.append(temp_out[1])\n        my_dictionary.extend(temp_out[0])\n        \n#Again tokenize the string\nsentence_token = [[words.text for words in sp(cl_sent) if words.text not in [' ', '  ']] for cl_sent in data1]\n    \n# print(sentence_token[0:10])\n# print(my_dictionary[0:10])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets write the hyperparameter for model \nembedding_size = 60\nwindow_size = 40\nmin_word = 5\ndown_sampling = 1e-2\n\nfrom gensim.models.fasttext import FastText\n\nft_model = FastText(sentence_token,\n                      size=embedding_size,\n                      window=window_size,\n                      min_count=min_word,\n                      sample=down_sampling,\n                      sg=1,\n                      iter=100)\n\nft_model.wv.save_word2vec_format('model.bin')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#create a word count dictionary\nword_count = {}\n\nfor x in my_dictionary:\n    if x not in ['',' ',]:\n        str1 = str(x).lower()\n        if str1 in word_count.keys():\n            word_count[str1] = word_count[str1] + 1\n        else:\n            word_count[str1] = 1\n\n\nfor sn,d in enumerate(word_count.items()):\n    k,v = d\n    if sn <10:\n        print(k,v)\n\nwords = word_count\nlen(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# original from https://github.com/cbaziotis/ekphrasis/blob/master/ekphrasis/classes/spellcorrect.py\n# improved it\n\nimport re\nfrom collections import Counter\n\nclass SpellCorrector:\n    \"\"\"\n    The SpellCorrector extends the functionality of the Peter Norvig's\n    spell-corrector in http://norvig.com/spell-correct.html\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        :param corpus: the statistics from which corpus to use for the spell correction.\n        \"\"\"\n        super().__init__()\n        self.WORDS = words\n        self.N = sum(self.WORDS.values())\n        \n    @staticmethod\n    def tokens(text):\n        return REGEX_TOKEN.findall(text.lower())\n\n    def P(self, word):\n        \"\"\"\n        Probability of `word`.\n        \"\"\"\n        return self.WORDS[word] / self.N\n\n    def most_probable(self, words):\n        _known = self.known(words)\n        if _known:\n            return max(_known, key=self.P)\n        else:\n            return []\n\n    @staticmethod\n    def edit_step(word):\n        \"\"\"\n        All edits that are one edit away from `word`.\n        \"\"\"\n        letters = 'abcdefghijklmnopqrstuvwxyz'\n        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n        deletes = [L + R[1:] for L, R in splits if R]\n        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n        inserts = [L + c + R for L, R in splits for c in letters]\n        \n        return set(deletes + transposes + replaces + inserts)\n\n    def edits2(self, word):\n        \"\"\"\n        All edits that are two edits away from `word`.\n        \"\"\"\n        return (e2 for e1 in self.edit_step(word)\n                for e2 in self.edit_step(e1))\n\n    def known(self, words):\n        \"\"\"\n        The subset of `words` that appear in the dictionary of WORDS.\n        \"\"\"\n        return set(w for w in words if w in self.WORDS)\n\n    def edit_candidates(self, word, assume_wrong=False, fast=True):\n        \"\"\"\n        Generate possible spelling corrections for word.\n        \"\"\"\n\n        if fast:\n            ttt = self.known(self.edit_step(word)) or {word}\n        else:\n            ttt = self.known(self.edit_step(word)) or self.known(self.edits2(word)) or {word}\n        \n        ttt = self.known([word]) | ttt\n        return list(ttt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"corrector = SpellCorrector()\npossible_states = corrector.edit_candidates('wh')\npossible_states\nft_model.wv.most_similar(['wh'], topn=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import spacy\nimport re\nsp = spacy.load('en_core_web_sm')\nall_stopwords = sp.Defaults.stop_words\n\n#This class checks the context word if given and use the word vector for getting the contexually correct suggestion.\n    \ndef score_normalise(sorted_dict):\n    \n    n_s_d = {}\n    for i in sorted_dict:\n        k,v = i\n        dict_length = len(words)\n        frequency = words[k]/dict_length\n        factor = frequency*v\n        n_s_d[k] = factor\n    #print('n_s_d',n_s_d)\n    return n_s_d\n        \n\ndef context_word(un_word):\n    full_out_str = []\n    \n    \n    for word2 in un_word:\n       \n        corrector = SpellCorrector()\n        \n        #get the candidate for this error\n        possible_states = corrector.edit_candidates(word2)\n        if possible_states[0] == word2:\n            possible_states = []\n        \n        #get the possible states\n        m_out = [(item[0],item[1]) for item in ft_model.wv.most_similar([word2], topn=3)]\n        \n        for f in m_out:   \n            if f[1] >= 0.74:\n                possible_states.append(f[0])\n        \n        score_dict = {}\n        #print('ps',possible_states)\n        \n        if len(possible_states) > 0:\n            \n            \n            for c in list(set(possible_states)):\n                score = ft_model.wv.similarity(w1=word2, w2=c)\n                score_dict[c] = score\n\n            sorted_dict = [(k,v) for k,v in sorted(score_dict.items(),key = lambda x:x[1])]\n            \n            n_score_dict = score_normalise(sorted_dict)\n            \n            n_score_dict = [(k,v) for k,v in sorted(n_score_dict.items(),key = lambda x:x[1])]\n            \n            if len(n_score_dict)>0:\n                out_str = n_score_dict[-1][0]\n            else:\n                out_str = word2\n        else:\n            out_str = word2\n\n        #print('out for word is',out_str)\n        full_out_str.append(out_str)\n        \n    return full_out_str\n    \n    \n    \n\ndef no_context(word1):\n    \n    word1 = word1[0]\n    out_str = []\n    \n    #get the candidate for this error\n    corrector = SpellCorrector()\n    possible_states = corrector.edit_candidates(word1)\n    \n    \n    if possible_states[0] == word1:\n        \n        possible_states = []\n    \n    model_o = [(item[0],item[1]) for item in ft_model.wv.most_similar([word1], topn=3)]\n    \n    for f in model_o:\n        if f[1] > 0.74:\n            possible_states.append(f[0])\n            \n    #print(possible_states)\n    score_dict = {}\n    \n    if len(set(possible_states)) == 1:\n        out_str = possible_states   \n        \n    else:  \n        for c in list(set(possible_states)):\n            \n            score = ft_model.wv.similarity(w1=word1, w2=c)\n            score_dict[c] = score\n            \n        sorted_dict = [(k,v) for k,v in sorted(score_dict.items(),key = lambda x:x[1])]\n        \n        n_score_dict = score_normalise(sorted_dict)\n        \n        n_score_dict = [(k,v) for k,v in sorted(n_score_dict.items(),key = lambda x:x[1])]\n        \n        \n        if len(n_score_dict)>0:\n            out_str = [n_score_dict[-1][0]]\n        else:\n            out_str = [word1]\n        \n    return out_str\n        \n        \n    \ndef test_model(test_str):\n    \n    sp = spacy.load('en_core_web_sm')\n    all_stopwords = sp.Defaults.stop_words\n    \n    test_word = [w for w in sp(test_str)]\n    \n    un_words = []\n    \n    un_lemma_word = []\n    \n    known_word = []\n    \n    for w1 in test_word:\n        \n        if w1.text.lower() not in all_stopwords:\n            \n            #get the lemma \n            w_lemma = w1.lemma_\n            if w_lemma.lower() not in word_count.keys():\n                un_words.append(w1.text)\n                un_lemma_word.append(w_lemma.lower()) \n            else:\n                known_word.append(w_lemma.lower())\n                \n            \n    \n    text_msk = ''\n    unknown_word = []\n    output_str = ''\n    output_list = []\n    \n    \n    \n    if len(un_lemma_word) > 0: \n        \n        #check if length:\n        if len(test_str) <= 2:\n            output_list = no_context(un_words)\n\n\n        if len(test_str) > 2:\n            output_list = context_word(un_words)\n        \n        #print('output is ',output_list)\n        \n        #substitute the value\n        for i in range(len(un_words)):\n            \n            #print(un_words[i],output_list[i])\n            test_str = re.sub(un_words[i],output_list[i],test_str)\n            \n    print('corrected string is -> ', test_str)\n            \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here you can test the model using your own string\n\ntest_str = 'whre'\n\nprint('test_string is ->',test_str)\n\ntest_model(test_str.strip(' '))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}